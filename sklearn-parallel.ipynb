{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Supposedly: Run this block first, then sbatch the exported .slurm file through the console\n",
    "### But it doesn't actually work due to errors activating conda through the cluster(?)\n",
    "### For now, skip this block\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "print(multiprocessing.cpu_count()) #num of available cpus\n",
    "\n",
    "import os\n",
    "os.system('jupyter nbconvert --to python sklearn-parallel.ipynb') # Converts the nb into .py\n",
    "\n",
    "job_name = \"rfr_job\"\n",
    "sbatch_file = f\"{job_name}.slurm\"\n",
    "\n",
    "### Creating a sbatch file to run the .py \n",
    "with open(sbatch_file, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\")\n",
    "    f.write(f\"#SBATCH --job-name={job_name}\\n\")\n",
    "    f.write(\"#SBATCH --output=output.out\\n\")\n",
    "    f.write(\"#SBATCH --account=csd709\\n\") # Change account number to new one\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-node=10\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=5\\n\")\n",
    "    f.write(\"#SBATCH --time=01:30:00\\n\")\n",
    "    f.write(\"#SBATCH --mem=32G\\n\")\n",
    "    f.write(\"#SBATCH --partition=shared\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"module purge\\n\")\n",
    "    f.write(\"module load slurm\\n\")\n",
    "    f.write(\"module load cpu/0.15.4  gcc/10.2.0\\n\")\n",
    "    #f.write(\"module load gpu/0.15.4\\n\") \n",
    "    f.write(\"module load python/3.8.5\\n\")\n",
    "    f.write(\"module load singularitypro/3.7\\n\")\n",
    "    f.write(\"module load anaconda3/2020.11/conda\\n\")\n",
    "    f.write(\"echo $CONDA_PYTHON_EXE\\n\")\n",
    "    f.write(\"conda activate base\\n\")\n",
    "    f.write(\"\\n\")    \n",
    "    f.write(\"python ./sklearn-parallel.py\\n\")\n",
    "# Python outputs should be found in the *.out file  \n",
    "\n",
    "os.system(f\"sbatch {sbatch_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  9.12392689e+05,  1.68597525e+06, ...,\n",
       "         2.78461900e+12,  1.00358797e+12,  2.97558333e+11],\n",
       "       [-5.47779602e-12,  5.35972333e+13,  8.17158826e+13, ...,\n",
       "         1.16331140e+09,  8.23480169e+08,  4.26582636e+08],\n",
       "       [ 0.00000000e+00,  1.68268296e+13,  2.62041737e+13, ...,\n",
       "         3.31027987e+10,  3.98052117e+10,  2.57056280e+10],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  4.23774406e+15,  5.88839228e+15, ...,\n",
       "        -8.42365120e+10, -5.29676771e+10, -3.64623675e+10],\n",
       "       [ 0.00000000e+00,  3.96142323e+12,  6.22212473e+12, ...,\n",
       "         1.09171263e+12, -1.63494357e+12, -3.20546057e+11],\n",
       "       [ 4.32462462e-18,  4.07106353e+08,  7.05768695e+08, ...,\n",
       "         7.50076315e+10,  1.21111702e+11,  2.06857976e+11]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90, 301)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 301)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(90, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 301)\n"
     ]
    }
   ],
   "source": [
    "#### Data preperation\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Load and preprocess the data\n",
    "folder_path = '100 files'\n",
    "data = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            df1 = pd.read_csv(file_path, header=None, names=['des_flux', 'detrap_en', 'def_conc'], index_col=None, squeeze = True)\n",
    "            #df1 = df1.fillna(df1.mean()) # Replace NaN with column average\n",
    "            data[filename] = df1\n",
    "            \n",
    "# Combine all the data into a single dataframe\n",
    "df = pd.concat(data.values())\n",
    "\n",
    "des_flux=df['des_flux']\n",
    "des_flux=des_flux.tolist()\n",
    "num_files=int(len(des_flux)/301)\n",
    "des_flux=np.reshape(des_flux, [num_files, 301])\n",
    "display(des_flux)\n",
    "#print(type(des_flux[0]), des_flux.shape)\n",
    "\n",
    "detrap_en=df['detrap_en']\n",
    "detrap_en=detrap_en.dropna()\n",
    "detrap_en=detrap_en.tolist()\n",
    "num_files=int(len(detrap_en)/4)\n",
    "detrap_en=np.reshape(detrap_en, [num_files, 4])\n",
    "#print(detrap_en, detrap_en.shape)\n",
    "\n",
    "def_conc=df['def_conc']\n",
    "def_conc=def_conc.dropna()\n",
    "def_conc=def_conc.tolist()\n",
    "num_files=int(len(def_conc)/4)\n",
    "def_conc=np.reshape(def_conc, [num_files, 4])\n",
    "#print(def_conc, def_conc.shape)\n",
    "\n",
    "def_param = np.concatenate([detrap_en, def_conc], axis=1)\n",
    "print(def_param.shape)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test= train_test_split(des_flux, test_size=0.1, random_state=42)\n",
    "y_train, y_test = train_test_split(def_param, test_size=0.1, random_state=42)\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_norm = scaler.fit_transform(X_train.reshape(-1, 1))\n",
    "X_test_norm = scaler.transform(X_test.reshape(-1, 1))\n",
    "\n",
    "#Reshape data for model input\n",
    "#Checked to make sure the time-series nature of the X_train, X_test is intact, can double check\n",
    "#By calling an array and comparing with a .csv file\n",
    "X_train_transform = np.reshape(X_train_norm, (X_train.shape))\n",
    "print(X_train_transform.shape)\n",
    "\n",
    "X_test_transform = np.reshape(X_test_norm, (X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.tree import plot_tree\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import parallel_backend\n",
    "\n",
    "### Defining a function to train a random forest with gridsearchcv\n",
    "def train_rf(X, y, i):\n",
    "    rfr = RandomForestRegressor(random_state=42, max_depth=10)\n",
    "    param_grid= {\"n_estimators\": range(1,50), \"max_depth\": range(1,20)}\n",
    "    folds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    \n",
    "    with parallel_backend('threading'): # threading for inner loop\n",
    "        rf_gcv = GridSearchCV(rfr, param_grid, \n",
    "                              return_train_score=True,\n",
    "                              scoring=\"neg_mean_squared_error\",\n",
    "                              cv=folds, n_jobs=50)\n",
    "    \n",
    "        rf_gcv.fit(X, y)\n",
    "\n",
    "    print(rf_gcv.best_params_)\n",
    "    print(rf_gcv.best_score_)\n",
    "    \n",
    "    # Predict based on test data \n",
    "    best_gcv = rf_gcv.best_estimator_\n",
    "    rfr_y_predict = best_gcv.predict(X)\n",
    "\n",
    "    # Calculate the mse\n",
    "    mse = mean_squared_error(y, rfr_y_predict)\n",
    "\n",
    "    # Print the MSE for this model\n",
    "    print(f\"Model {i} MSE: {mse}\")\n",
    "\n",
    "    return rf_gcv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Method 1\n",
    "# Define the number of models to train in parallel\n",
    "num_models = 10\n",
    "\n",
    "# Parallelize the model training using sklearn joblib\n",
    "models = Parallel(n_jobs=num_models)(\n",
    "    delayed(train_rf)(X_train_transform, y_train, i) for i in range(num_models))\n",
    "\n",
    "# Combine the predictions from all the models\n",
    "y_pred = np.mean([model.predict(X_test_transform) for model in models], axis=0)\n",
    "\n",
    "# Calculate the final mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the final MSE\n",
    "print(f\"Final MSE = {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 9 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 2 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 4 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 0 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 6 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 1 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 8 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 3 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 5 MSE: 0.0010520446377357594\n",
      "{'max_depth': 7, 'n_estimators': 49}\n",
      "-0.006440878423247848\n",
      "Model 7 MSE: 0.0010520446377357594\n",
      "Final MSE = 0.006309377060303989\n",
      "CPU times: user 157 ms, sys: 130 ms, total: 287 ms\n",
      "Wall time: 13min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Method 2\n",
    "num_models = 10\n",
    "with parallel_backend('multiprocessing', n_jobs=-1): # multiprocessing for outloop, prevents the parallel nested loop error\n",
    "    models = Parallel()(delayed(train_rf)(X_train_transform, y_train, i) for i in range(num_models))\n",
    "\n",
    "# Combines predictions from all the models by taking the mean\n",
    "y_pred = np.mean([model.predict(X_test_transform) for model in models], axis=0)\n",
    "\n",
    "# Calculates final MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Prints final MSE\n",
    "print(f\"Final MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.007196555368356303\n"
     ]
    }
   ],
   "source": [
    "### Basic PLSR\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "plsr = PLSRegression(n_components=2)\n",
    "plsr.fit(X_train_transform, y_train)\n",
    "X_train_pls = plsr.transform(X_train_transform)\n",
    "pls_pred = plsr.predict(X_test_transform)\n",
    "# Calculates mse \n",
    "mse = mean_squared_error(y_test, pls_pred)\n",
    "print(f\"MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
