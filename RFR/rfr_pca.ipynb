{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import multiprocessing\n",
        "print(\"num of cpus:\", multiprocessing.cpu_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PTV0iokUXNq",
        "outputId": "926b2806-5997-4a69-ec05-e4ddfbddc514"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "num of cpus: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G2N5HtWULxE"
      },
      "outputs": [],
      "source": [
        "#### Data preperation\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/TDS_data_15000'\n",
        "\n",
        "#Load and preprocess the data\n",
        "def load_data(folder_path):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  data = {}\n",
        "  for filename in os.listdir(folder_path):\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "      if os.path.isfile(file_path):\n",
        "          with open(file_path, 'r') as file:\n",
        "              df1 = pd.read_csv(file_path, header=None, names=['des_flux', 'detrap_en', 'def_conc'], index_col=None, squeeze = True)\n",
        "              #df1 = df1.fillna(df1.mean()) # Replace NaN with column average\n",
        "              data[filename] = df1\n",
        "    \n",
        "              num_datapts = len(df1[\"des_flux\"])\n",
        "  # Combine all the data into a single dataframe\n",
        "  df = pd.concat(data.values())\n",
        "\n",
        "  return df, num_datapts\n",
        "\n",
        "df, num_datapts = load_data(folder_path)\n",
        "\n",
        "des_flux=df['des_flux']\n",
        "des_flux=des_flux.tolist()\n",
        "num_files=int(len(des_flux)/num_datapts) \n",
        "des_flux=np.reshape(des_flux, [num_files, num_datapts])\n",
        "    # This assumes that all spectra in the dataset/folder have the same\n",
        "    # amount of datapoints\n",
        "display(des_flux)\n",
        "#print(type(des_flux[0]), des_flux.shape)\n",
        "\n",
        "detrap_en=df['detrap_en']\n",
        "detrap_en=detrap_en.dropna()\n",
        "detrap_en=detrap_en.tolist()\n",
        "num_files=int(len(detrap_en)/4)\n",
        "detrap_en=np.reshape(detrap_en, [num_files, 4])\n",
        "#print(detrap_en, detrap_en.shape)\n",
        "\n",
        "def_conc=df['def_conc']\n",
        "def_conc=def_conc.dropna()\n",
        "def_conc=def_conc.tolist()\n",
        "num_files=int(len(def_conc)/4)\n",
        "def_conc=np.reshape(def_conc, [num_files, 4])\n",
        "#print(def_conc, def_conc.shape)\n",
        "\n",
        "def_param = np.concatenate([detrap_en, def_conc], axis=1)\n",
        "print(def_param.shape)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test= train_test_split(des_flux, test_size=0.1, random_state=42)\n",
        "y_train, y_test = train_test_split(def_param, test_size=0.1, random_state=42)\n",
        "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_train_norm = scaler.fit_transform(X_train.reshape(-1, 1))\n",
        "X_test_norm = scaler.transform(X_test.reshape(-1, 1))\n",
        "\n",
        "#Reshape data for model input\n",
        "#Checked to make sure the time-series nature of the X_train, X_test is intact, can double check\n",
        "#By calling an array and comparing with a .csv file\n",
        "X_train_transform = np.reshape(X_train_norm, (X_train.shape))\n",
        "print(X_train_transform.shape)\n",
        "\n",
        "X_test_transform = np.reshape(X_test_norm, (X_test.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4VyBQHkkULxG"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.tree import plot_tree\n",
        "from joblib import Parallel, delayed, parallel_backend#, print_progress\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### Defining a function to train a random forest with gridsearchcv\n",
        "# Attempted to use a PLS for dimensionality reduction, but I couldn't\n",
        "# get it to work with RFR, so switched to PCA instead\n",
        "def train_rf(X_train, y_train, X_test, y_test, i, n_estimators=None, max_depth=None):\n",
        "    \"\"\"Trains a random forest regression model using preprocessed training data.\n",
        "    Without specifying parameters, will preform a gridsearch across a range.\n",
        "    \"\"\"\n",
        "    folds = KFold(n_splits=10, shuffle=True)\n",
        "    rfr_y_pred = [] \n",
        "    if n_estimators is not None and max_depth is not None:\n",
        "        train_rf = Pipeline([('pca', PCA(n_components=2)), ('rfr', RandomForestRegressor(n_estimators=n_estimators,\n",
        "                                                                                         max_depth=max_depth))])    \n",
        "        pca = train_rf.named_steps['pca'] \n",
        "        pca.fit(X_train, y_train)                                                                 \n",
        "        X_train_transformed = pca.transform(X_train)[:, :2]\n",
        "        X_test_transformed = pca.transform(X_test)[:, :2]\n",
        "        train_rf.fit(X_train_transformed, y_train)\n",
        "        rfr_y_pred = train_rf.predict(X_test_transformed)\n",
        "    else:\n",
        "        param_grid= {\"rfr__n_estimators\": range(1,100), \"rfr__max_depth\": range(1,50)}\n",
        "        with parallel_backend('threading', n_jobs=-1): # threading to avoid nested loop error\n",
        "          train_rf = Pipeline([('pca', PCA(n_components=2)), ('rfr', RandomForestRegressor())]) \n",
        "          gcv = GridSearchCV(train_rf, param_grid, \n",
        "                           return_train_score=True,\n",
        "                           scoring=\"neg_mean_squared_error\",\n",
        "                           cv=folds, n_jobs=20)\n",
        "          pca = train_rf.named_steps['pca']\n",
        "          pca.fit(X_train, y_train)\n",
        "          X_train_transformed = pca.transform(X_train)[:, :2]\n",
        "          X_test_transformed = pca.transform(X_test)[:, :2]\n",
        "          gcv.fit(X_train_transformed, y_train)\n",
        "\n",
        "        print(gcv.best_params_)\n",
        "        print(gcv.best_score_)\n",
        "        \n",
        "        # Predict based on test data \n",
        "        train_rf = gcv.best_estimator_\n",
        "        rfr_y_pred = train_rf.predict(X_test_transformed)\n",
        "    \n",
        "        # Calculate the mse \n",
        "    mse = mean_squared_error(y_test, rfr_y_pred)\n",
        "\n",
        "    # Print the MSE for this model\n",
        "    #print(f\"Model {i} MSE: {mse}\")\n",
        "\n",
        "    return train_rf, rfr_y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3K/s, 15K dataset without noise"
      ],
      "metadata": {
        "id": "XG8TOssopnR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHXdUbFDULxH"
      },
      "outputs": [],
      "source": [
        "# https://joblib.readthedocs.io/en/latest/parallel.html\n",
        "num_models = 5 # set number of models to run in parallel\n",
        "\n",
        "with parallel_backend('multiprocessing', n_jobs=-1): # multiprocessing for outloop, prevents the parallel nested loop error\n",
        "    models = Parallel()(delayed(train_rf)(X_train_transform, y_train, X_test_transform, y_test, i, n_estimators=None, max_depth=None) for i in range(num_models))\n",
        "\n",
        "    # best_params = {'max_depth': 7, 'n_estimators': 49} \n",
        "    \n",
        "# Combines predictions from all the models by taking the mean\n",
        "y_pred = [model[1] for model in models]\n",
        "y_pred_mean = np.mean(y_pred, axis=0)\n",
        "\n",
        "# Calculates final MSE\n",
        "mse = mean_squared_error(y_test, y_pred_mean)\n",
        "\n",
        "# Prints final MSE\n",
        "print(f\"Final Test MSE = {mse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niSsr5ZCULxH"
      },
      "outputs": [],
      "source": [
        "### Adding noise\n",
        "import matplotlib.pyplot as plt\n",
        "def add_noise(des_flux):\n",
        "  spect_wnoise=[]\n",
        "  noise_array=[]\n",
        "  sigma_list=[]\n",
        "  for n in np.arange(0, len(des_flux)):\n",
        "      sigma_rand = np.random.uniform(1E14, 1E17)\n",
        "      for i, k in enumerate(des_flux[n]):\n",
        "          spect_wnoise.append(np.random.normal(k, sigma_rand))\n",
        "      sigma_list.append(str(sigma_rand))\n",
        "      noise_array.append(spect_wnoise)\n",
        "      spect_wnoise=[]\n",
        "  return noise_array\n",
        "#plt.plot(noise_array[4])\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prI93ZEMULxI"
      },
      "outputs": [],
      "source": [
        "# Setting test and train data using noisy spectra\n",
        "noise_array = add_noise(des_flux)\n",
        "\n",
        "X_train, X_test= train_test_split(noise_array, test_size=0.1, random_state=42)\n",
        "y_train, y_test = train_test_split(def_param, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "# Change lists into array\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_train_norm = scaler.fit_transform(np.array(X_train).reshape(-1, 1))\n",
        "X_test_norm = scaler.transform(np.array(X_test).reshape(-1, 1))\n",
        "\n",
        "#Reshape data for model input\n",
        "#Checked to make sure the time-series nature of\n",
        "#the X_train, X_test is intact, can double check\n",
        "#By calling an array and comparing with a .csv file\n",
        "X_train_transform = np.reshape(X_train_norm, (np.array(X_train).shape[0],-1))\n",
        "print(X_train_transform.shape)\n",
        "\n",
        "X_test_transform = np.reshape(X_test_norm, (np.array(X_test).shape[0],-1))\n",
        "print(X_test_transform.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3K/s, 15K dataset with noise"
      ],
      "metadata": {
        "id": "eZtYaBdqpYsa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac-eFyMxULxI",
        "outputId": "b1c42068-29b4-4411-97c7-4f27b2acc8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final MSE = 0.007430363362989451\n",
            "CPU times: user 322 ms, sys: 212 ms, total: 534 ms\n",
            "Wall time: 7.72 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "num_models = 5 # set number of models to run in parallel\n",
        "\n",
        "with parallel_backend('multiprocessing', n_jobs=-1): # multiprocessing for outloop, prevents the parallel nested loop error\n",
        "    noise_models = Parallel()(delayed(train_rf)(X_train_transform, y_train, X_test_transform, y_test, i, n_estimators=None, max_depth=None) for i in range(num_models))\n",
        "\n",
        "# Combines predictions from all the models by taking the mean\n",
        "y_pred = [model[1] for model in noise_models]\n",
        "y_pred_mean = np.mean(y_pred, axis=0)\n",
        "\n",
        "# Prints final MSE\n",
        "mse = mean_squared_error(y_test, y_pred_mean)\n",
        "print(f\"Final MSE = {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df, num_datapts = load_data(\"/content/drive/MyDrive/TDS_combined_raw\")\n",
        "\n",
        "des_flux=df['des_flux']\n",
        "des_flux=des_flux.tolist()\n",
        "num_files=int(len(des_flux)/num_datapts) \n",
        "des_flux=np.reshape(des_flux, [num_files, num_datapts])\n",
        "    # This assumes that all spectra in the dataset/folder have the same\n",
        "    # amount of datapoints\n",
        "display(des_flux)\n",
        "#print(type(des_flux[0]), des_flux.shape)\n",
        "\n",
        "detrap_en=df['detrap_en']\n",
        "detrap_en=detrap_en.dropna()\n",
        "detrap_en=detrap_en.tolist()\n",
        "num_files=int(len(detrap_en)/4)\n",
        "detrap_en=np.reshape(detrap_en, [num_files, 4])\n",
        "#print(detrap_en, detrap_en.shape)\n",
        "\n",
        "def_conc=df['def_conc']\n",
        "def_conc=def_conc.dropna()\n",
        "def_conc=def_conc.tolist()\n",
        "num_files=int(len(def_conc)/4)\n",
        "def_conc=np.reshape(def_conc, [num_files, 4])\n",
        "#print(def_conc, def_conc.shape)\n",
        "\n",
        "def_param = np.concatenate([detrap_en, def_conc], axis=1)\n",
        "print(def_param.shape)\n",
        "\n",
        "noise_array = add_noise(des_flux)\n",
        "\n",
        "X_train, X_test= train_test_split(noise_array, test_size=0.1, random_state=42)\n",
        "y_train, y_test = train_test_split(def_param, test_size=0.1, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "# Change lists into array\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_train_norm = scaler.fit_transform(np.array(X_train).reshape(-1, 1))\n",
        "X_test_norm = scaler.transform(np.array(X_test).reshape(-1, 1))\n",
        "\n",
        "#Reshape data for model input\n",
        "X_train_transform = np.reshape(X_train_norm, (np.array(X_train).shape[0],-1))\n",
        "print(X_train_transform.shape)\n",
        "X_test_transform = np.reshape(X_test_norm, (np.array(X_test).shape[0],-1))\n",
        "print(X_test_transform.shape)\n"
      ],
      "metadata": {
        "id": "qfgdL3wXbrNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with parallel_backend('multiprocessing', n_jobs=-1): # multiprocessing for outloop, prevents the parallel nested loop error\n",
        "    noise_models = Parallel()(delayed(train_rf)(X_train_transform, y_train, X_test_transform, y_test, i, n_estimators=None, max_depth=None) for i in range(num_models))\n",
        "\n",
        "# Combines predictions from all the models by taking the mean\n",
        "y_pred = [model[1] for model in noise_models]\n",
        "y_pred_mean = np.mean(y_pred, axis=0)\n",
        "\n",
        "# Prints final MSE\n",
        "mse = mean_squared_error(y_test, y_pred_mean)\n",
        "print(f\"Final MSE = {mse}\")\n"
      ],
      "metadata": {
        "id": "lnhrku3xsDBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}